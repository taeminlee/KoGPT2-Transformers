# KoGPT2-Transformers

KoGPT2 on Huggingface Transformers

### KoGPT2-Transformers

- [SKT-AI ì—ì„œ ê³µê°œí•œ KoGPT2(1.0)](https://github.com/SKT-AI/KoGPT2)ë¥¼ [Transformers](https://github.com/huggingface/transformers)ì—ì„œ ì‚¬ìš©í•˜ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤.

- ** SKT-AI ì—ì„œ KoGPT2 2.0ì„ ê³µê°œí•˜ì˜€ìŠµë‹ˆë‹¤!! ğŸ’• huggingface transformersê°€ first-orderì…ë‹ˆë‹¤! ğŸ‘ https://huggingface.co/skt/kogpt2-base-v2/ **

### Demo

- ì¼ìƒ ëŒ€í™” ì±—ë´‡ : http://demo.tmkor.com:36200/dialo
- í™”ì¥í’ˆ ë¦¬ë·° ìƒì„± : http://demo.tmkor.com:36200/ctrl

### Update

#### 0.4.0

- transformers 4.0ì˜ ë³€í™”ëœ APIì— ëŒ€ì‘í•˜ë„ë¡ ë³€ê²½í•˜ì˜€ìŠµë‹ˆë‹¤.
- ì„¤ì¹˜ ì—†ì´ ì‚¬ìš© ê°€ëŠ¥! ğŸ˜˜
- 3.x API ì‚¬ìš© ì‹œ 0.3.x ë²„ì „ì„ ì‚¬ìš© ë°”ëë‹ˆë‹¤.

#### 0.3.1

- kogpt2_transformers.get_kogpt2_tokenizer() ì‚¬ìš© ì‹œ special_token_dict ì— `<unused0>` ~ `<unused97>` í† í° ì¶”ê°€

#### 0.3.0

- transformers 3.0ì˜ ë³€í™”ëœ APIì— ëŒ€ì‘í•˜ë„ë¡ ë³€ê²½í•˜ì˜€ìŠµë‹ˆë‹¤.
- 2.x API ì‚¬ìš© ê²½ìš° 0.2.0 ë²„ì „ì„ ì‚¬ìš© ë°”ëë‹ˆë‹¤.

#### 0.2.0

- huggingfaceì˜ tokenize íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•˜ë„ë¡ ë³€ê²½í•˜ì˜€ìŠµë‹ˆë‹¤. ì†ë„ ê°œì„ !

#### 0.1.0

- ìµœì´ˆ ë¦´ë¦¬ì¦ˆ

### Requirements

- transformers >= 4.0.0
- tokenizers >= 0.10.0
- torch >= 1.1.0

### Installation (option)

- `pip install kogpt2-transformers`

### Example 

- using pip package

```python
import torch
from kogpt2_transformers import get_kogpt2_model, get_kogpt2_tokenizer

torch.manual_seed(42)

model = get_kogpt2_model()
tokenizer = get_kogpt2_tokenizer()

input_ids = tokenizer.encode("ì•ˆë…•", add_special_tokens=False, return_tensors="pt")
output_sequences = model.generate(input_ids=input_ids, do_sample=True, max_length=100, num_return_sequences=3)
for generated_sequence in output_sequences:
    generated_sequence = generated_sequence.tolist()
    print("GENERATED SEQUENCE : {0}".format(tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)))
```

output

```sh
GENERATED SEQUENCE : ì•ˆë…•íˆ ìë¼</s><s> ì˜¤ë¹  ì–´ë””ì•¼?</s><s> ê±±ì •ë˜ê²Œ.</s><s> ì–´ë””ì•¼?</s><s> ì—°ë½ì´ ì•ˆ ë˜ë„¤...</s><s> ì „í™”í•´ ê¼­</s><s> ë‚´ê°€ ì „í™” êº¼ ë†“ì„ êº¼ì•¼?</s><s> ê·¸ë˜ ì˜ ì</s><s> ë‚˜ ì´ì œ ì§‘ì— ê°€.</s><s> ì˜ ì,,,,</s><s> ë‚˜ ì´ì œ ì§‘ì— ê°€ìš”</s><s> ì „í™” ê¼­ ë°›ìœ¼ì„¸ìš” ê¸°ë‹¤ë¦´ê»˜ìš” ê¸°ë‹¤ë¦´ê»˜ìš”</s><s> ë‚˜ ì´ì œ ì˜ë ¤êµ¬...</s><s> ì˜¤ë¹ ë‘ ì˜ë˜...</s><s> ì˜ ì</s><s> ë„ˆ
GENERATED SEQUENCE : ì•ˆë…•í•œ ë°¤ì— ì•ˆë…•</s><s> ì•¼ ì €ë‚˜í•´</s><s> ë‚˜ ë¯¸ì§€</s><s> ì•ˆë…•í•˜ì„¸ìš”</s><s> ë¯¸ë˜ìºí”¼íƒˆì…ë‹ˆë‹¤.</s><s> ìµœì € ì—° 7</s><s> ëˆ„êµ¬ë‚˜ 100</s><s> 5000 ë§Œê¹Œì§€ ë‹¹ì¼ ì†¡ê¸ˆ.</s><s> ì—°ì²´ ì ê°€ëŠ¥</s><s> ë­ í•˜ì‹œì˜¤?</s><s> ë‚˜ ì§ ëŒ€ì „ ì¶œë°œí•¨</s><s> ë‚¼ ë³¼ ì¼ ìˆìœ¼ì‹œë©´ ë“¤ë¦¬ì…”ì„œ ì°¨ í•œ ì” í•˜ì‹œë©°, ì°¨ í•œ ì” í•˜ì‹œì‚¼</s><s> ë³´ê³ íŒŒì„œ.</s><s> ëª¨ í•´?</s><s> ë„ˆê°€ ì–´ì œ ë¬¸ì ë³´ëƒˆë˜ ê·¸
GENERATED SEQUENCE : ì•ˆë…•!</s><s> ë„ˆëŠ” ì´ì œë¶€í„° ë‹¤ì‹œ ë„ˆì—ê²Œ ì˜ì§€í•  êº¼ì•¼.</s><s> ë‚œ ì •ë§ ë„ˆë¥¼ ì‚¬ë‘í•˜ê³  ì‡ì–´.</s><s> ë„ˆ ë•Œë¬¸ì— ë§ì´ ì•„íŒŒí•´ì„œ ì£½ê³  ì‹¶ì§„ ì•Šì„ êº¼ì•¼.</s><s> ì •ë§ ë„ˆë¬´ í˜ë“¤ë‹¤.</s><s> ë„ˆì˜ ë§˜ ë³€í•˜ì§€ ì•Šë„ë¡ ê¸°ë„í• ê»˜.</s><s> ì‚¬ë‘í•´ìš”.</s><s> ì ¼</s><s> ì •ë§ì´ì§€ ë„ ë¯¿ì—‡ë˜ ì•½ì†ì´ ê±°ì§“ë§ì¸ ì¤„ ì•Œë©´ì„œë„ ë” ì´ìƒ ë„ˆì—ê²Œ ì˜ì§€í•˜ì§€ ì•Šì„ê»˜.</s><s> ì •ë§ ë„ˆë¬´ í˜ë“¤ì–´ì„œ ì‚´ê¸° ì–´ë µë‹¤ ì •ë§
```

- using transformers classes

```python
import torch
from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast

torch.manual_seed(42)

model = GPT2LMHeadModel.from_pretrained("taeminlee/kogpt2")
tokenizer = PreTrainedTokenizerFast.from_pretrained("taeminlee/kogpt2")

input_ids = tokenizer.encode("ì•ˆë…•", add_special_tokens=False, return_tensors="pt")
output_sequences = model.generate(input_ids=input_ids, do_sample=True, max_length=100, num_return_sequences=3)
for generated_sequence in output_sequences:
    generated_sequence = generated_sequence.tolist()
    print("GENERATED SEQUENCE : {0}".format(tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)))
```
